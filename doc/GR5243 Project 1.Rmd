---
title: "An EDA on the Causes of Happiness"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---
![](/Users/bennet/Downloads/Study at Columbia/3.jpg)
\newline
\newline
\newline
```{r message=FALSE, warning=F}
library(tm)
library(tidytext)
library(tidyverse)
library(DT)
library(ggplot2)
library(wordcloud)
library(topicmodels) 
library(ngram)
```

Step 1 - Load the data to be cleaned and processed

```{r read data, warning=FALSE, message=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile)
```

Step 2 - Preliminary cleaning of text
\newline
We clean the text by converting all the letters to the lower case, and removing punctuation, numbers, empty words and extra white space.
```{r text processing in tm}
corpus <- VCorpus(VectorSource(hm_data$cleaned_hm))%>%
  tm_map(content_transformer(tolower))%>%
  tm_map(removePunctuation)%>%
  tm_map(removeNumbers)%>%
  tm_map(removeWords, character(0))%>%
  tm_map(stripWhitespace)
```

Step 3 - Stemming words and converting tm object to tidy object
\newline
Stemming reduces a word to its word *stem*. We stem the words here and then convert the "tm" object to a "tidy" object for much faster processing.

```{r stemming}
stemmed <- tm_map(corpus, stemDocument) %>%
  tidy() %>%
  select(text)
```

Step 4 - Creating tidy format of the dictionary to be used for completing stems
\newline
We also need a dictionary to look up the words corresponding to the stems.
```{r tidy dictionary}
dict <- tidy(corpus) %>%
  select(text) %>%
  unnest_tokens(dictionary, text)
```

Step 5 - Removing stopwords that don't hold any significant information for our data set
\newline
We remove stopwords provided by the "tidytext" package and also add custom stopwords in context of our data.
```{r stopwords}
data("stop_words")

word <- c("happy","ago","yesterday","lot","today","months","month",
                 "happier","happiest","last","week","past")

stop_words <- stop_words %>%
  bind_rows(mutate(tibble(word), lexicon = "updated"))
```

Step 6 - Combining stems and dictionary into the same tibble
\newline
Here we combine the stems and the dictionary into the same "tidy" object.
```{r tidy stems with dictionary}
completed <- stemmed %>%
  mutate(id = row_number()) %>%
  unnest_tokens(stems, text) %>%
  bind_cols(dict) %>%
  anti_join(stop_words, by = c("dictionary" = "word"))
```

Step 7 - Stem completion
\newline
Lastly, we complete the stems by picking the corresponding word with the highest frequency.
```{r stem completion, warning=FALSE, message=FALSE}
completed <- completed %>%
  group_by(stems) %>%
  count(dictionary) %>%
  mutate(word = dictionary[which.max(n)]) %>%
  ungroup() %>%
  select(stems, word) %>%
  distinct() %>%
  right_join(completed) %>%
  select(-stems)
```

Step 8 - Pasting stem completed individual words into their respective happy moments
\newline
We want our processed words to resemble the structure of the original happy moments. So we paste the words together to form happy moments.
```{r reverse unnest}
completed <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(word, collapse = " ")) %>%
  ungroup()
```

Step 9 - Keeping a track of the happy moments with their own ID
```{r cleaned hm_data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  mutate(id = row_number()) %>%
  inner_join(completed)
```

```{r load data, warning=FALSE, message=FALSE}
urlfile<-'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demo_data <- read_csv(urlfile)
```

Combine both the data sets and keep the required columns for analysis
\newline
We select a subset of the data that satisfies specific row conditions.
```{r combining data, warning=FALSE, message=FALSE}
hm_data <- hm_data %>%
  inner_join(demo_data, by = "wid") %>%
  select(wid,
         original_hm,
         gender, 
         marital, 
         parenthood,
         reflection_period,
         age, 
         country, 
         ground_truth_category, 
         text) %>%
  mutate(count = sapply(hm_data$text, wordcount)) %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))
```

```{r bag of words, warning=FALSE, message=FALSE}
bag_of_words <-  hm_data %>%
  unnest_tokens(word, text)

word_count <- bag_of_words %>%
  count(word, sort = TRUE)
```
\newline
![](/Users/bennet/Downloads/Study at Columbia/1.jpg)
 
###Begining of EDA
  
####1 Overview of Data
Many things can make one's heart smile with joy. Today we will delve into the causes of someone being happiness in terms of parenthood by using [HappyDB](https://rit-public.github.io/HappyDB/) database. After we cleaned our data and combined it with extra more demographical information, we first take a look at the overall commonly used words to get a abstract vision of our text file.
\newline

```{r warning=F}
wordcloud(word_count$word, freq = word_count$n,min.freq = 1000,max.words = 100,
          random.order = F,rot.per = 0.25,colors = brewer.pal(8,"Dark2"),scale = c(4,0.5))
```
\newline
\newline
\newline
 
 
 
 
 
 
 
 
 
 
From the word cloud map, we can see roughly, the causes of happiness is some how 'friend', 'time' and 'day'. Then we could try to visualize it in bar plot to have an idea on what are the most frequently used words after we filtered out those meaningfuless words.
```{r}
ggplot(word_count[1:50,])+
  geom_bar(mapping = aes(x = word,y=n,alpha = n),stat = "identity")+
  coord_flip()
```
\newline
\newline
\newline
 
 
 
 
 
So overall speaking, besides those terms, 'home', 'family' and 'watched' seems to be also pervasive. Now let's divide our data into two parts to see whether having a child is influencing the causes of happiness.
 
 
####2 Group data according to parenthood for sentiment analysis
```{r}
data_by_prt <- hm_data %>%
  select(text,parenthood)%>%
  unnest_tokens(word,text)
```
\newline
We can see that in the following map, people who are not parents, their source of being pleased is due to 'time with friend' somehow.
```{r}
data_by_prt %>%
  filter(parenthood == "n")%>%
  VectorSource()%>%
  VCorpus()%>%
  wordcloud(min.freq = 500,max.words = 100,
          random.order = F,rot.per = 0.25,colors = brewer.pal(8,"Dark2"),scale = c(4,0.5))
```
\newline
\newline
\newline
 
 
 
 
 
 
 
 
 
 
 
For people who are parents already, we can see that the words 'son','daughter','husband' and 'school' things show up more frequently, maybe they are enjoying being a good farther and mother!
```{r}
data_by_prt %>%
  filter(parenthood == "y")%>%
  VectorSource()%>%
  VCorpus()%>%
  wordcloud(min.freq = 500,max.words = 100,
          random.order = F,rot.per = 0.25,colors = brewer.pal(8,"Dark2"),scale = c(4,0.5))
```
\newline
\newline
\newline
 
 
 
 
 
 
 
 
Now let us do sentiment analysis on two groups of  people. It is revealed that people with kids their positive rates is much higher than those who are not parents.
```{r}
data_by_prt%>%
  filter(parenthood == "n")%>%
  inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
  count(sentiment) %>% # count the # of positive & negative words
  spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
  mutate(sentiment = positive - negative) # # of positive words - # of negative owrds

data_by_prt%>%
  filter(parenthood == "y")%>%
  inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
  count(sentiment) %>% # count the # of positive & negative words
  spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
  mutate(sentiment = positive - negative) # # of positive words - # of negative owrds
```
\newline
 

####3 Group data according to parenthood for topic modeling
Let us do topic modeling to have a closer look at the causes of happiness.
```{r}
DTM <- data_by_prt%>%
  filter(parenthood=="n")%>%
  VectorSource()%>%
  VCorpus()%>%
  DocumentTermMatrix(VCorpus(VectorSource(data_by_prt)))
inspect(DTM)
unique_indexes <- unique(DTM$i) # get the index of each unique value
DTM <- DTM[unique_indexes,] # get a subset of only those indexes
lda <- LDA(DTM, k = 6, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
top_terms <- topics  %>% # take the topics data frame and..
  group_by(topic) %>% # treat each topic as a different group
  top_n(10, beta) %>% # get the top 10 most informative words
  ungroup() %>% # ungroup
  arrange(topic, -beta) # arrange words in descending informativeness

top_terms %>% # take the top terms
  mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
  ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
  geom_col(show.legend = FALSE) + # as a bar plot
  facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
  labs(x = NULL, y = "Beta") + # no x label, change y label 
  coord_flip() # turn bars sideways
```

```{r}
DTM <- data_by_prt%>%
  filter(parenthood=="y")%>%
  VectorSource()%>%
  VCorpus()%>%
  DocumentTermMatrix(VCorpus(VectorSource(data_by_prt)))
inspect(DTM)
unique_indexes <- unique(DTM$i) # get the index of each unique value
DTM <- DTM[unique_indexes,] # get a subset of only those indexes
lda <- LDA(DTM, k = 6, control = list(seed = 1234))
topics <- tidy(lda, matrix = "beta")
top_terms <- topics  %>% # take the topics data frame and..
  group_by(topic) %>% # treat each topic as a different group
  top_n(10, beta) %>% # get the top 10 most informative words
  ungroup() %>% # ungroup
  arrange(topic, -beta) # arrange words in descending informativeness

top_terms %>% # take the top terms
  mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
  ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
  geom_col(show.legend = FALSE) + # as a bar plot
  facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
  labs(x = NULL, y = "Beta") + # no x label, change y label 
  coord_flip() # turn bars sideways

```
\newline
 
 
 
 
 
 
 
 
 
From the above 12 charts, we can tell that for people who do not have kids to take care, the commonly most informative words are 'freind', 'day', 'time' 'game' and  'job'. As for people who are parents, we can see from the charts, the most informative words are 'son', 'daughter', 'school', 'husband', as well as 'friend'. However this seems tobe definitely more family-oriented.
 
 
 
![](/Users/bennet/Downloads/Study at Columbia/2.jpg)
 
 
 
 
####4 Summay
There are countless reasons why people could be happy, however the most ones are things like a day spent with someone special ones could be a friend or family.
 
There are difference in terms of being happy between group of parents and non-parents. For the people, who are not parents yet, their causes of hapiness seem tobe more social-activity oriented, as one can see the informative words including:friend, day, time etc. As for the parents group, it reveals that their causes of happiness are more family-oriented, informative words like: son, daughter, school, family, home, husband and wife. 
 
Combing the setiment analysis EDA, we could find that people with kids are more likely to report in a positive emotion, which means having kids, at least on average, can improve the happiness, and maybe this kind of conclusion could be used in developed country to inflate the birth rate.



